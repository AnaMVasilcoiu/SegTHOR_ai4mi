{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11c9a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from scipy.ndimage import binary_erosion\n",
    "\n",
    "def evaluate_3d_metrics(pred_path, gt_path, num_classes):\n",
    "    # Load the predicted and ground truth NIfTI files\n",
    "    pred_nii = nib.load(pred_path)\n",
    "    gt_nii = nib.load(gt_path)\n",
    "    \n",
    "    # Get the data arrays from the NIfTI objects\n",
    "    pred_volume = pred_nii.get_fdata()\n",
    "    gt_volume = gt_nii.get_fdata()\n",
    "    \n",
    "    print(\"Unique values in prediction volume:\", np.unique(pred_volume))\n",
    "    print(\"Unique values in ground truth volume:\", np.unique(gt_volume))\n",
    "    \n",
    "    # Create a mapping dictionary for the predicted labels\n",
    "    mapping = {0: 0, 63: 1, 126: 2, 189: 3, 252: 4}\n",
    "\n",
    "    # Apply the mapping to the predicted volume\n",
    "    remapped_pred_volume = np.copy(pred_volume)\n",
    "    for original_value, new_value in mapping.items():\n",
    "        remapped_pred_volume[pred_volume == original_value] = new_value\n",
    "        \n",
    "    pred_volume = remapped_pred_volume\n",
    "    \n",
    "    print(\"Unique values in prediction volume:\", np.unique(pred_volume))\n",
    "    print(\"Unique values in ground truth volume:\", np.unique(gt_volume))\n",
    "    \n",
    "    \n",
    "    print(pred_volume.shape)\n",
    "\n",
    "    # Check shape\n",
    "    assert pred_volume.shape == gt_volume.shape, \"Shape mismatch between prediction and ground truth volumes.\"\n",
    "\n",
    "    metrics = {}\n",
    "    # Skip background (class 0)\n",
    "    for c in range(1, num_classes):  \n",
    "        # print(c)\n",
    "        pred_mask = (pred_volume == c).astype(np.uint8)\n",
    "        gt_mask = (gt_volume == c).astype(np.uint8)\n",
    "        \n",
    "        \n",
    "        # Compute 3D IoU\n",
    "        intersection = np.sum(pred_mask & gt_mask)\n",
    "        union = np.sum(pred_mask | gt_mask)\n",
    "        iou = intersection / union if union != 0 else 0\n",
    "        metrics[f\"IoU_Class_{c}\"] = iou\n",
    "\n",
    "        print(\"IOU\", iou)\n",
    "\n",
    "        # Compute 3D Dice Score\n",
    "        dice = (2 * intersection) / (np.sum(pred_mask) + np.sum(gt_mask)) if (np.sum(pred_mask) + np.sum(gt_mask)) != 0 else 0\n",
    "        metrics[f\"Dice_Class_{c}\"] = dice\n",
    "\n",
    "        print(\"Dice\", dice)\n",
    "\n",
    "        # Compute 3D Hausdorff Distance\n",
    "        pred_boundary = pred_mask - binary_erosion(pred_mask)\n",
    "        gt_boundary = gt_mask - binary_erosion(gt_mask)\n",
    "        pred_boundary_points = np.argwhere(pred_boundary)\n",
    "        gt_boundary_points = np.argwhere(gt_boundary)\n",
    "        \n",
    "        if pred_boundary_points.size > 0 and gt_boundary_points.size > 0:\n",
    "            hd_pred_to_gt = directed_hausdorff(pred_boundary_points, gt_boundary_points)[0]\n",
    "            hd_gt_to_pred = directed_hausdorff(gt_boundary_points, pred_boundary_points)[0]\n",
    "            hausdorff_distance = max(hd_pred_to_gt, hd_gt_to_pred)\n",
    "        else:\n",
    "            hausdorff_distance = np.inf\n",
    "\n",
    "        metrics[f\"Hausdorff_Class_{c}\"] = hausdorff_distance\n",
    "        print(\"Hausdorff\", hausdorff_distance)\n",
    "\n",
    "        if c != 0:\n",
    "            # Compute 3D ASSD (Average Symmetric Surface Distance)\n",
    "            if pred_boundary_points.size > 0 and gt_boundary_points.size > 0:\n",
    "                assd_pred_to_gt = np.mean([np.min(np.linalg.norm(pred_boundary_points - point, axis=1)) for point in gt_boundary_points])\n",
    "                assd_gt_to_pred = np.mean([np.min(np.linalg.norm(gt_boundary_points - point, axis=1)) for point in pred_boundary_points])\n",
    "                assd = (assd_pred_to_gt + assd_gt_to_pred) / 2\n",
    "            else:\n",
    "                assd = np.inf\n",
    "            \n",
    "            metrics[f\"ASSD_Class_{c}\"] = assd\n",
    "            print(\"ASSD\", assd)\n",
    "\n",
    "        # Compute 3D Volumetric Similarity\n",
    "        gt_vol = np.sum(gt_mask, dtype=np.float64)\n",
    "        pred_vol = np.sum(pred_mask, dtype=np.float64)\n",
    "\n",
    "        vol_sim = 1 - abs(gt_vol - pred_vol) / (gt_vol + pred_vol) if (gt_vol + pred_vol) != 0 else 0\n",
    "        metrics[f\"VolSim_Class_{c}\"] = vol_sim\n",
    "        print(\"VOL Sim\", vol_sim)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in prediction volume: [  0.  63. 126. 189. 252.]\n",
      "Unique values in ground truth volume: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_all_metrics(pred_folder, gt_folder, num_classes):\n",
    "    # Find all prediction files in the prediction folder\n",
    "    pred_files = glob.glob(os.path.join(pred_folder, '*.nii.gz'))\n",
    "    \n",
    "    # List to store metrics for each patient\n",
    "    metrics_list = []\n",
    "    \n",
    "    for pred_file in pred_files:\n",
    "        # Extract the patient ID from the prediction file name\n",
    "        patient_id = os.path.basename(pred_file).split('.')[0]\n",
    "        \n",
    "        # Construct the corresponding ground truth file path\n",
    "        gt_file = os.path.join(gt_folder, patient_id, 'GT_corrected.nii.gz')\n",
    "        \n",
    "        # Check if the ground truth file exists\n",
    "        if os.path.exists(gt_file):\n",
    "            # Evaluate metrics for this patient\n",
    "            metrics = evaluate_3d_metrics(pred_file, gt_file, num_classes)\n",
    "            # Add the patient ID to the metrics\n",
    "            metrics['Patient_ID'] = patient_id\n",
    "            # Append metrics to the list\n",
    "            metrics_list.append(metrics)\n",
    "        else:\n",
    "            print(f\"Ground truth file not found for {patient_id}. Skipping.\")\n",
    "    \n",
    "    # Convert the metrics list to a DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "# Example usage\n",
    "pred_folder = 'volumes/segthor/results_AdamW_K=25_best_epoch/'\n",
    "gt_folder = 'data/segthor_train/train/'\n",
    "metrics_df = evaluate_all_metrics(pred_folder, gt_folder, num_classes=5)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv('metrics_results_results_AdamW_K=25_best_epoch.csv', index=False)\n",
    "\n",
    "# Calculate averages, min, and max for each metric across all patients\n",
    "summary_stats = metrics_df.describe().loc[['mean', 'min', 'max']]\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Save summary statistics to a CSV file\n",
    "summary_stats.to_csv('metrics_summary_results_AdamW_K=25_best_epoch.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb5c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
